# Infrastructure Setup Guide

This document details the complete infrastructure setup in the `model` namespace, including Llama Stack, MCP Server, and model hosting.

---

## üì¶ Overview

The infrastructure consists of 4 main components deployed in the `model` namespace:

```
model namespace
‚îú‚îÄ‚îÄ LlamaStack Distribution    (Llama Stack server with Granite embeddings)
‚îú‚îÄ‚îÄ OCP MCP Server             (Kubernetes/OpenShift operations server)
‚îú‚îÄ‚îÄ BGE Reranker               (Document reranking model via vLLM)
‚îî‚îÄ‚îÄ Llama 3.2 3B Instruct      (LLM for generation via vLLM)
```

---

## üèóÔ∏è Component 1: Llama Stack Distribution

### Purpose
Provides Llama Stack API for:
- **Embedding generation** (Granite 125M model)
- **Vector storage** (Milvus local DB)
- **Model management**

### Configuration

**Image:** `quay.io/opendatahub/llama-stack:odh`

**Key Environment Variables:**
```yaml
HF_HOME: /.llama
VLLM_URL: https://llama-32-3b-instruct-predictor.model.svc.cluster.local:8443/v1
INFERENCE_MODEL: llama-32-3b-instruct
VLLM_TLS_VERIFY: false
VLLM_API_TOKEN: <JWT token from service account>
MILVUS_DB_PATH: ~/.llama/milvus.db
```

**Storage:**
- **PVC:** `llamastack-custom-distribution-pvc` (for models and Milvus DB)
- **Mount:** `/.llama`

**Service:**
- **Port:** 8321
- **Route:** `llamastack-custom-distribution-service-model.apps.rosa.loki123.orwi.p3.openshiftapps.com`

### Deployment YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamastack-custom-distribution
  namespace: model
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack
  template:
    metadata:
      labels:
        app: llama-stack
    spec:
      containers:
      - name: llama-stack
        image: quay.io/opendatahub/llama-stack:odh
        ports:
        - containerPort: 8321
        env:
        - name: HF_HOME
          value: /.llama
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-inference-model-secret
              key: VLLM_URL
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-inference-model-secret
              key: INFERENCE_MODEL
        - name: VLLM_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              name: llama-stack-inference-model-secret
              key: VLLM_TLS_VERIFY
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-inference-model-secret
              key: VLLM_API_TOKEN
        - name: MILVUS_DB_PATH
          value: ~/.llama/milvus.db
        volumeMounts:
        - name: lls-storage
          mountPath: /.llama
      volumes:
      - name: lls-storage
        persistentVolumeClaim:
          claimName: llamastack-custom-distribution-pvc
```

### Secret Configuration

Create the secret (replace `<BASE64_ENCODED_VALUES>`):

```bash
oc create secret generic llama-stack-inference-model-secret \
  --from-literal=VLLM_URL='https://llama-32-3b-instruct-predictor.model.svc.cluster.local:8443/v1' \
  --from-literal=INFERENCE_MODEL='llama-32-3b-instruct' \
  --from-literal=VLLM_TLS_VERIFY='false' \
  --from-literal=VLLM_API_TOKEN='<SERVICE_ACCOUNT_TOKEN>' \
  -n model
```

**To get SERVICE_ACCOUNT_TOKEN:**
```bash
# The token is auto-generated by the Llama model's service account
oc get secret -n model | grep llama-32-3b-instruct-sa
oc get secret <secret-name> -n model -o jsonpath='{.data.token}' | base64 -d
```

---

## üèóÔ∏è Component 2: OCP MCP Server

### Purpose
Provides MCP (Model Context Protocol) server for Kubernetes/OpenShift operations:
- Pod management
- Log fetching
- Resource inspection
- Events monitoring

### Configuration

**Image:** `quay.io/njajodia/ocp-mcp-server:latest`

**Command:**
```bash
./kubernetes-mcp-server --sse-port 8000
```

**Service Account:** `ocp-mcp` (with cluster-wide read permissions)

**Port:** 8000 (SSE - Server-Sent Events)

### Deployment YAML

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ocp-mcp
  namespace: model
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ocp-mcp-reader
rules:
- apiGroups: [""]
  resources:
  - pods
  - pods/log
  - events
  - namespaces
  - services
  - configmaps
  - secrets
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources:
  - deployments
  - replicasets
  - statefulsets
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ocp-mcp-reader-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ocp-mcp-reader
subjects:
- kind: ServiceAccount
  name: ocp-mcp
  namespace: model
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ocp-mcp-server
  namespace: model
  labels:
    app: ocp-mcp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ocp-mcp-server
  template:
    metadata:
      labels:
        app: ocp-mcp-server
    spec:
      serviceAccountName: ocp-mcp
      containers:
      - name: ocp-mcp-server
        image: quay.io/njajodia/ocp-mcp-server:latest
        command: ["./kubernetes-mcp-server"]
        args: ["--sse-port", "8000"]
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: ocp-mcp-server
  namespace: model
spec:
  selector:
    app: ocp-mcp-server
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
```

### How to Deploy

```bash
# Apply the complete MCP server setup
oc apply -f infrastructure/ocp-mcp-server.yaml

# Verify deployment
oc get pods -n model -l app=ocp-mcp-server
oc logs -n model -l app=ocp-mcp-server

# Test MCP server
oc port-forward -n model svc/ocp-mcp-server 8000:8000
# Then test: curl http://localhost:8000/health
```

---

## üèóÔ∏è Component 3: BGE Reranker v2-m3

### Purpose
Reranks retrieved documents for better relevance using BAAI's BGE Reranker v2-m3 model.

### Configuration

**Model:** `BAAI/bge-reranker-v2-m3`  
**Storage:** `oci://quay.io/njajodia/bge-reranker-modelcar:v2` (OCI model container)  
**Runtime:** `nvidia-reranker-vllm` (vLLM with reranking support)  
**Max Context:** 512 tokens  

**Resources:**
- **GPU:** 1x NVIDIA GPU
- **CPU:** 1-2 cores
- **Memory:** 4-8Gi

**vLLM Args:**
```bash
--port=8080
--model=/mnt/models
--served-model-name=bge-reranker
--task=score                    # Reranking task
--enforce-eager
--max-model-len=512             # BGE's limit
--dtype=half                    # FP16 for efficiency
--trust-remote-code
```

### InferenceService YAML

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: bge-reranker
  namespace: model
  annotations:
    openshift.io/display-name: "BGE Reranker v2-m3 (BAAI)"
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: "RawDeployment"
  labels:
    opendatahub.io/dashboard: "true"
    networking.kserve.io/visibility: "exposed"
spec:
  predictor:
    automountServiceAccountToken: false
    minReplicas: 1
    maxReplicas: 1
    model:
      modelFormat:
        name: vLLM
      runtime: nvidia-reranker-vllm
      storageUri: oci://quay.io/njajodia/bge-reranker-modelcar:v2
      args:
      - --port=8080
      - --model=/mnt/models
      - --served-model-name=bge-reranker
      - --task=score
      - --enforce-eager
      - --max-model-len=512
      - --dtype=half
      - --trust-remote-code
      resources:
        requests:
          cpu: "1"
          memory: 4Gi
          nvidia.com/gpu: "1"
        limits:
          cpu: "2"
          memory: 8Gi
          nvidia.com/gpu: "1"
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
```

### Deployment

```bash
# Apply InferenceService
oc apply -f infrastructure/bge-reranker-inferenceservice.yaml

# Wait for model to load (~2-3 minutes)
oc get inferenceservice bge-reranker -n model -w

# Check status
oc get pods -n model -l serving.kserve.io/inferenceservice=bge-reranker

# Test reranker
BGE_URL=$(oc get route bge-reranker -n model -o jsonpath='{.spec.host}')
curl -X POST https://${BGE_URL}/score \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the error?",
    "documents": ["Error: pod failed", "Pod is running"]
  }'
```

### Endpoint

**Internal:** `http://bge-reranker-predictor.model.svc.cluster.local:8080`  
**External:** `https://bge-reranker-model.apps.rosa.loki123.orwi.p3.openshiftapps.com`

---

## üèóÔ∏è Component 4: Llama 3.2 3B Instruct

### Purpose
Main LLM for:
- Document grading
- Answer generation
- Query transformation

### Configuration

**Model:** `meta-llama/Llama-3.2-3B-Instruct`  
**Storage:** `oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct`  
**Runtime:** `llama-32-3b-instruct` (vLLM)  
**Max Context:** 20,000 tokens  

**Resources:**
- **GPU:** 1x NVIDIA GPU
- **CPU:** 1 core
- **Memory:** 10-20Gi

**vLLM Args:**
```bash
--port=8080
--model=/mnt/models
--served-model-name=llama-32-3b-instruct
--dtype=half
--max-model-len=20000           # Extended context
--gpu-memory-utilization=0.95
--disable-custom-all-reduce
--enforce-eager
--enable-auto-tool-choice       # Tool calling support
--tool-call-parser=llama3_json
--chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja
```

### InferenceService YAML

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-32-3b-instruct
  namespace: model
  annotations:
    openshift.io/display-name: "llama-3.2-3b-instruct"
    security.opendatahub.io/enable-auth: "true"
    serving.kserve.io/deploymentMode: "RawDeployment"
  labels:
    opendatahub.io/dashboard: "true"
    networking.kserve.io/visibility: "exposed"
spec:
  predictor:
    automountServiceAccountToken: false
    minReplicas: 1
    maxReplicas: 1
    model:
      modelFormat:
        name: vLLM
      runtime: llama-32-3b-instruct
      storageUri: oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct
      args:
      - --port=8080
      - --model=/mnt/models
      - --served-model-name=llama-32-3b-instruct
      - --dtype=half
      - --max-model-len=20000
      - --gpu-memory-utilization=0.95
      - --disable-custom-all-reduce
      - --enforce-eager
      - --enable-auto-tool-choice
      - --tool-call-parser=llama3_json
      - --chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja
      resources:
        requests:
          cpu: "1"
          memory: 10Gi
          nvidia.com/gpu: "1"
        limits:
          cpu: "1"
          memory: 20Gi
          nvidia.com/gpu: "1"
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
```

### Deployment

```bash
# Apply InferenceService
oc apply -f infrastructure/llama-32-3b-instruct-inferenceservice.yaml

# Wait for model to load (~5-10 minutes, model is 6GB)
oc get inferenceservice llama-32-3b-instruct -n model -w

# Check status
oc get pods -n model -l serving.kserve.io/inferenceservice=llama-32-3b-instruct

# Test LLM
LLAMA_URL=$(oc get route llama-32-3b-instruct -n model -o jsonpath='{.spec.host}')
TOKEN=$(oc create token llama-32-3b-instruct-sa -n model)

curl -X POST https://${LLAMA_URL}/v1/chat/completions \
  -H "Authorization: Bearer ${TOKEN}" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-32-3b-instruct",
    "messages": [{"role": "user", "content": "What is OpenShift?"}],
    "max_tokens": 100
  }'
```

### Endpoint

**Internal:** `https://llama-32-3b-instruct-predictor.model.svc.cluster.local:8443`  
**External:** `https://llama-32-3b-instruct-model.apps.rosa.loki123.orwi.p3.openshiftapps.com`

---

## üîÑ Complete Deployment Sequence

### 1. Prerequisites

```bash
# Ensure you have:
- OpenShift cluster with GPU nodes
- OpenShift AI (RHOAI) or ODH installed
- KServe/ModelMesh configured
- GPU operator installed
- StorageClass for PVCs

# Verify GPU availability
oc get nodes -l nvidia.com/gpu.present=true
```

### 2. Create Namespace

```bash
oc create namespace model
oc project model
```

### 3. Deploy Llama 3.2 3B (First - needed by Llama Stack)

```bash
oc apply -f infrastructure/llama-32-3b-instruct-inferenceservice.yaml

# Wait for ready
oc wait --for=condition=Ready inferenceservice/llama-32-3b-instruct -n model --timeout=600s
```

### 4. Deploy BGE Reranker

```bash
oc apply -f infrastructure/bge-reranker-inferenceservice.yaml

# Wait for ready
oc wait --for=condition=Ready inferenceservice/bge-reranker -n model --timeout=300s
```

### 5. Deploy Llama Stack

```bash
# Get service account token for Llama model
SA_TOKEN=$(oc create token llama-32-3b-instruct-sa -n model --duration=87600h)

# Create secret
oc create secret generic llama-stack-inference-model-secret \
  --from-literal=VLLM_URL='https://llama-32-3b-instruct-predictor.model.svc.cluster.local:8443/v1' \
  --from-literal=INFERENCE_MODEL='llama-32-3b-instruct' \
  --from-literal=VLLM_TLS_VERIFY='false' \
  --from-literal=VLLM_API_TOKEN="${SA_TOKEN}" \
  -n model

# Create PVC for Llama Stack
oc apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llamastack-custom-distribution-pvc
  namespace: model
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
EOF

# Deploy Llama Stack
oc apply -f infrastructure/llamastack-deployment.yaml

# Wait for ready
oc wait --for=condition=Available deployment/llamastack-custom-distribution -n model --timeout=300s
```

### 6. Deploy OCP MCP Server

```bash
oc apply -f infrastructure/ocp-mcp-server.yaml

# Wait for ready
oc wait --for=condition=Available deployment/ocp-mcp-server -n model --timeout=120s
```

### 7. Verify Everything

```bash
# Check all pods
oc get pods -n model

# Expected output:
# NAME                                            READY   STATUS    RESTARTS   AGE
# bge-reranker-predictor-xxx                      1/1     Running   0          5m
# llama-32-3b-instruct-predictor-xxx              1/1     Running   0          10m
# llamastack-custom-distribution-xxx              1/1     Running   0          3m
# ocp-mcp-server-xxx                              1/1     Running   0          1m

# Check all services
oc get svc -n model

# Check routes
oc get routes -n model
```

---

## üß™ Testing the Infrastructure

### Test Llama Stack

```bash
LLAMA_STACK_URL=$(oc get route llamastack-custom-distribution-service -n model -o jsonpath='{.spec.host}')

# Test embeddings
curl -X POST https://${LLAMA_STACK_URL}/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Hello world",
    "model": "granite-embedding-125m"
  }'
```

### Test BGE Reranker

```bash
BGE_URL=$(oc get route bge-reranker -n model -o jsonpath='{.spec.host}')

curl -X POST https://${BGE_URL}/score \
  -H "Content-Type: application/json" \
  -d '{
    "query": "pod error",
    "documents": [
      "Pod is running normally",
      "Error: CrashLoopBackOff detected"
    ]
  }'
```

### Test Llama LLM

```bash
LLAMA_URL=$(oc get route llama-32-3b-instruct -n model -o jsonpath='{.spec.host}')
TOKEN=$(oc create token llama-32-3b-instruct-sa -n model)

curl -X POST https://${LLAMA_URL}/v1/chat/completions \
  -H "Authorization: Bearer ${TOKEN}" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-32-3b-instruct",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Explain Kubernetes in one sentence."}
    ],
    "max_tokens": 100
  }'
```

### Test MCP Server

```bash
# Port forward
oc port-forward -n model svc/ocp-mcp-server 8000:8000 &

# Test health
curl http://localhost:8000/health

# Test pod listing (via MCP)
curl http://localhost:8000/pods?namespace=default
```

---

## üìä Resource Requirements

### Total Cluster Resources Needed

```
GPUs:     2x NVIDIA (1 for Llama, 1 for BGE)
CPU:      6+ cores
Memory:   40+ Gi
Storage:  50+ Gi (PVCs + ephemeral)
```

### Per-Component Breakdown

| Component | CPU | Memory | GPU | Storage |
|-----------|-----|--------|-----|---------|
| Llama 3.2 3B | 1 | 10-20Gi | 1 | - |
| BGE Reranker | 1-2 | 4-8Gi | 1 | - |
| Llama Stack | 0.5 | 2Gi | 0 | 20Gi PVC |
| MCP Server | 0.5 | 1Gi | 0 | - |
| **Total** | **3-4** | **17-31Gi** | **2** | **20Gi** |

---

## üîß Troubleshooting

### Llama Stack Issues

```bash
# Check logs
oc logs -n model deployment/llamastack-custom-distribution

# Common issues:
# 1. Cannot connect to Llama model
#    ‚Üí Verify secret has correct VLLM_URL and token
#    ‚Üí Check: oc get secret llama-stack-inference-model-secret -n model -o yaml

# 2. Milvus DB errors
#    ‚Üí Check PVC is mounted: oc get pvc llamastack-custom-distribution-pvc -n model
#    ‚Üí Verify write permissions

# 3. Embedding model not loading
#    ‚Üí Check HF_HOME is set and writable
```

### BGE Reranker Issues

```bash
# Check predictor logs
oc logs -n model -l serving.kserve.io/inferenceservice=bge-reranker

# Common issues:
# 1. OOM (Out of Memory)
#    ‚Üí Increase memory limits
#    ‚Üí Reduce --max-model-len

# 2. GPU not available
#    ‚Üí Check: oc describe pod -n model <bge-pod>
#    ‚Üí Verify GPU operator is running
```

### Llama Model Issues

```bash
# Check predictor logs
oc logs -n model -l serving.kserve.io/inferenceservice=llama-32-3b-instruct

# Common issues:
# 1. Model download slow
#    ‚Üí Check network/proxy settings
#    ‚Üí Use pre-cached modelcar

# 2. GPU memory issues
#    ‚Üí Reduce --gpu-memory-utilization
#    ‚Üí Reduce --max-model-len
```

### MCP Server Issues

```bash
# Check logs
oc logs -n model deployment/ocp-mcp-server

# Common issues:
# 1. Permission denied
#    ‚Üí Verify ClusterRoleBinding exists
#    ‚Üí Check: oc get clusterrolebinding ocp-mcp-reader-binding

# 2. Cannot list pods
#    ‚Üí Verify service account: oc get sa ocp-mcp -n model
```

---

## üìö Related Documentation

- **README.md** - Project overview and quick start
- **ARCHITECTURE.md** - System design and agent workflow
- **FILE_GUIDE.md** - What each file does
- **CRITICAL_FIXES_APPLIED.md** - Technical fixes applied

---

## üîÑ Updating Infrastructure

### Update Llama Stack Image

```bash
oc set image deployment/llamastack-custom-distribution \
  llama-stack=quay.io/opendatahub/llama-stack:latest \
  -n model
```

### Update MCP Server Image

```bash
oc set image deployment/ocp-mcp-server \
  ocp-mcp-server=quay.io/njajodia/ocp-mcp-server:v2 \
  -n model
```

### Update Model Version

```bash
# Edit InferenceService
oc edit inferenceservice llama-32-3b-instruct -n model

# Update storageUri to new version
# Save and wait for rollout
```

---

**Last Updated:** October 17, 2025  
**Cluster:** ROSA (Red Hat OpenShift on AWS)  
**OpenShift AI:** ODH (Open Data Hub)

